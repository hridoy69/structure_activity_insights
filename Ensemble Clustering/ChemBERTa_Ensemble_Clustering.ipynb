{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c88071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def get_molecular_embeddings(smiles_list):\n",
    "    \"\"\"\n",
    "    Calculate molecular embeddings for a list of SMILES strings using ChemBERTa\n",
    "    \n",
    "    Parameters:\n",
    "    smiles_list (list): List of SMILES strings\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Normalized embeddings matrix\n",
    "    \"\"\"\n",
    "    # Load ChemBERTa model and tokenizer\n",
    "    model_name = \"seyonec/ChemBERTa-zinc-base-v1\"  # Using ChemBERTa instead of MolBERT\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)    \n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize list to store embeddings\n",
    "    embeddings_list = []\n",
    "    \n",
    "    # Process each SMILES string\n",
    "    with torch.no_grad():\n",
    "        for smiles in smiles_list:\n",
    "            try:\n",
    "                # Tokenize SMILES\n",
    "                inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "                \n",
    "                # Get model outputs\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                # Use CLS token embedding (first token)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "                embeddings_list.append(embedding.flatten())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing SMILES: {smiles}\")\n",
    "                print(f\"Error message: {str(e)}\")\n",
    "                embeddings_list.append(np.zeros(768))  # ChemBERTa base model has 768 dimensions\n",
    "    \n",
    "    embeddings_array = np.array(embeddings_list)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_embeddings = scaler.fit_transform(embeddings_array)\n",
    "    \n",
    "    return normalized_embeddings\n",
    "\n",
    "def process_drug_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process a CSV file containing drug SMILES and save embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    input_file (str): Path to input CSV file with SMILES column\n",
    "    output_file (str): Path to save output embeddings\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Processed data with embeddings\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    print(\"Reading CSV file...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    print(f\"Processing {len(df)} SMILES strings...\")\n",
    "    # Calculate embeddings\n",
    "    embeddings = get_molecular_embeddings(df['SMILES'].tolist())\n",
    "    \n",
    "    print(\"Creating embedding DataFrame...\")\n",
    "    # Create DataFrame with embeddings\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embeddings,\n",
    "        columns=[f'embedding_{i}' for i in range(embeddings.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Combine original data with embeddings\n",
    "    result_df = pd.concat([df, embedding_df], axis=1)\n",
    "    \n",
    "    print(f\"Saving results to {output_file}...\")\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"Dataset_2016.csv\"\n",
    "    output_file = r\"Drugs_With_Embedded.csv\"\n",
    "    \n",
    "    try:\n",
    "        result = process_drug_file(input_file, output_file)\n",
    "        print(\"Processing completed successfully!\")\n",
    "        print(f\"Shape of result: {result.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input file not found. Please check the file path.\")\n",
    "    except KeyError:\n",
    "        print(\"Error: 'SMILES' column not found in the CSV file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ff797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def normalize_embeddings(embeddings_array):\n",
    "    \"\"\"\n",
    "    Normalize embeddings to range [0,1] using Min-Max scaling\n",
    "    \n",
    "    Parameters:\n",
    "    embeddings_array (numpy.ndarray): Array of embeddings to normalize\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Normalized embeddings matrix\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_embeddings = scaler.fit_transform(embeddings_array)\n",
    "    \n",
    "    # Verify normalization\n",
    "    print(\"\\nVerifying normalization:\")\n",
    "    print(f\"Minimum value: {np.min(normalized_embeddings)}\")\n",
    "    print(f\"Maximum value: {np.max(normalized_embeddings)}\")\n",
    "    \n",
    "    return normalized_embeddings\n",
    "\n",
    "def get_molecular_embeddings(smiles_list):\n",
    "    \"\"\"\n",
    "    Calculate molecular embeddings for a list of SMILES strings using ChemBERTa\n",
    "    \n",
    "    Parameters:\n",
    "    smiles_list (list): List of SMILES strings\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Normalized embeddings matrix\n",
    "    \"\"\"\n",
    "    # Load ChemBERTa model and tokenizer\n",
    "    print(\"Loading ChemBERTa model and tokenizer...\")\n",
    "    model_name = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize list to store embeddings\n",
    "    embeddings_list = []\n",
    "    \n",
    "    # Process each SMILES string\n",
    "    print(\"\\nGenerating embeddings...\")\n",
    "    with torch.no_grad():\n",
    "        for i, smiles in enumerate(smiles_list):\n",
    "            try:\n",
    "                # Tokenize SMILES\n",
    "                inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "                \n",
    "                # Get model outputs\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                # Use CLS token embedding (first token)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "                embeddings_list.append(embedding.flatten())\n",
    "                \n",
    "                # Print progress every 100 molecules\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(f\"Processed {i + 1}/{len(smiles_list)} molecules...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing SMILES: {smiles}\")\n",
    "                print(f\"Error message: {str(e)}\")\n",
    "                # Add a zero vector as embedding for failed cases\n",
    "                embeddings_list.append(np.zeros(768))  # ChemBERTa base model has 768 dimensions\n",
    "    \n",
    "    # Convert list to numpy array\n",
    "    print(\"\\nConverting to numpy array...\")\n",
    "    embeddings_array = np.array(embeddings_list)\n",
    "    \n",
    "    # Print shape and stats before normalization\n",
    "    print(\"\\nEmbeddings before normalization:\")\n",
    "    print(f\"Shape: {embeddings_array.shape}\")\n",
    "    print(f\"Min value: {np.min(embeddings_array)}\")\n",
    "    print(f\"Max value: {np.max(embeddings_array)}\")\n",
    "    print(f\"Mean value: {np.mean(embeddings_array)}\")\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    print(\"\\nNormalizing embeddings...\")\n",
    "    normalized_embeddings = normalize_embeddings(embeddings_array)\n",
    "    \n",
    "    return normalized_embeddings\n",
    "\n",
    "def process_drug_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process a CSV file containing drug SMILES and save embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    input_file (str): Path to input CSV file with SMILES column\n",
    "    output_file (str): Path to save output embeddings\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Processed data with embeddings\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    print(\"Reading CSV file...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    print(f\"\\nFound {len(df)} SMILES strings to process...\")\n",
    "    \n",
    "    # Check if SMILES column exists\n",
    "    if 'SMILES' not in df.columns:\n",
    "        raise KeyError(\"CSV file must contain a 'SMILES' column\")\n",
    "    \n",
    "    # Calculate embeddings\n",
    "    embeddings = get_molecular_embeddings(df['SMILES'].tolist())\n",
    "    \n",
    "    print(\"\\nCreating embedding DataFrame...\")\n",
    "    # Create DataFrame with embeddings\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embeddings,\n",
    "        columns=[f'embedding_{i}' for i in range(embeddings.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Combine original data with embeddings\n",
    "    result_df = pd.concat([df, embedding_df], axis=1)\n",
    "    \n",
    "    print(f\"\\nSaving results to {output_file}...\")\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"\\nFinal statistics:\")\n",
    "    print(f\"Original dataframe shape: {df.shape}\")\n",
    "    print(f\"Embedding dataframe shape: {embedding_df.shape}\")\n",
    "    print(f\"Final dataframe shape: {result_df.shape}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"r\"Dataset_2016.csv\"\"\n",
    "    output_file = r\"Drugs_With_Embedded_Bits.csv\"\n",
    "    \n",
    "    try:\n",
    "        result = process_drug_file(input_file, output_file)\n",
    "        print(\"\\nProcessing completed successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input file not found. Please check the file path.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bad377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.spatial.distance import cdist\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "import hdbscan\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "def optimize_clustering(X, algorithm):\n",
    "    \"\"\"Optimize clustering hyperparameters using Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        scaler_type = trial.suggest_categorical('scaler', ['standard', 'minmax', 'robust', 'quantile', 'power'])\n",
    "        scaler = {'standard': StandardScaler(), 'minmax': MinMaxScaler(), 'robust': RobustScaler(),\n",
    "                  'quantile': QuantileTransformer(), 'power': PowerTransformer()}[scaler_type]\n",
    "\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        min_pca = min(5, X_scaled.shape[1])  \n",
    "        max_pca = min(30, X_scaled.shape[1])  \n",
    "        if min_pca > max_pca:\n",
    "            min_pca = max_pca  \n",
    "\n",
    "        n_components = trial.suggest_int('n_components', min_pca, max_pca)\n",
    "        X_pca = PCA(n_components=n_components).fit_transform(X_scaled)\n",
    "\n",
    "        if algorithm == 'kmeans':\n",
    "            n_clusters = trial.suggest_int('n_clusters', 2, 10)\n",
    "            model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        elif algorithm == 'hdbscan':\n",
    "            min_cluster_size = trial.suggest_int('min_cluster_size', 5, 50)\n",
    "            model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "        elif algorithm == 'spectral':\n",
    "            n_clusters = trial.suggest_int('n_clusters', 2, 10)\n",
    "            graph = kneighbors_graph(X_pca, n_neighbors=15, include_self=False)\n",
    "            n_components_graph, _ = connected_components(graph)\n",
    "            if n_components_graph > 1:\n",
    "                return float('-inf')\n",
    "            model = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', random_state=42)\n",
    "        elif algorithm == 'agglomerative':\n",
    "            n_clusters = trial.suggest_int('n_clusters', 2, 10)\n",
    "            model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        elif algorithm == 'gmm':\n",
    "            n_components = trial.suggest_int('n_components', 2, 10)\n",
    "            model = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        else:\n",
    "            return float('-inf')\n",
    "\n",
    "        labels = model.fit_predict(X_pca)\n",
    "        if len(set(labels)) < 2:\n",
    "            return float('-inf')\n",
    "\n",
    "        return silhouette_score(X_pca, labels)\n",
    "\n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=15, timeout=120)\n",
    "    return study.best_params\n",
    "\n",
    "def optimize_linkage(co_assoc_matrix):\n",
    "    \"\"\"Optimize linkage method and number of clusters using Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        linkage_method = trial.suggest_categorical('linkage_method', ['single', 'complete', 'average', 'ward'])\n",
    "        linkage_matrix = linkage(1 - co_assoc_matrix, method=linkage_method)\n",
    "\n",
    "        max_clusters = 10\n",
    "        t = trial.suggest_int('n_clusters', 2, max_clusters)\n",
    "        labels = fcluster(linkage_matrix, t, criterion='maxclust')\n",
    "\n",
    "        if len(set(labels)) < 2:\n",
    "            return float('-inf')\n",
    "\n",
    "        return silhouette_score(1 - co_assoc_matrix, labels)\n",
    "\n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=15)\n",
    "    return study.best_params\n",
    "\n",
    "def dunn_index(X, labels):\n",
    "    unique_clusters = np.unique(labels)\n",
    "    n_clusters = len(unique_clusters)\n",
    "    if n_clusters < 2:\n",
    "        return -1  \n",
    "\n",
    "    intra_dists = []\n",
    "    inter_dists = []\n",
    "\n",
    "    for i in unique_clusters:\n",
    "        cluster_i = X[labels == i]\n",
    "        if cluster_i.shape[0] > 1:\n",
    "            intra_dists.append(np.max(cdist(cluster_i, cluster_i)))\n",
    "        for j in unique_clusters:\n",
    "            if i >= j:\n",
    "                continue\n",
    "            cluster_j = X[labels == j]\n",
    "            inter_dists.append(np.min(cdist(cluster_i, cluster_j)))\n",
    "\n",
    "    if not intra_dists or not inter_dists:\n",
    "        return -1\n",
    "    return np.min(inter_dists) / np.max(intra_dists)\n",
    "\n",
    "\n",
    "def perform_ensemble_clustering(csv_path):\n",
    "    \"\"\"Runs multiple clustering algorithms on drug embeddings.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    embedding_cols = [col for col in data.columns if col.startswith('embedding_')]\n",
    "    if not embedding_cols:\n",
    "        raise ValueError(\"No embedding columns found in the dataset.\")\n",
    "\n",
    "    print(f\"Found {len(embedding_cols)} embedding dimensions\")\n",
    "    X = data[embedding_cols].values\n",
    "\n",
    "    print(\"Starting ensemble clustering...\")\n",
    "    algorithms = ['kmeans', 'hdbscan', 'spectral', 'agglomerative', 'gmm']\n",
    "    cluster_labels_list = []\n",
    "\n",
    "    for algo in algorithms:\n",
    "        print(f\"\\nOptimizing {algo} clustering...\")\n",
    "        best_params = optimize_clustering(X, algo)\n",
    "        if best_params is None:\n",
    "            continue\n",
    "        print(f\"Best parameters for {algo}: {best_params}\")\n",
    "\n",
    "        if algo == 'kmeans':\n",
    "            model = KMeans(n_clusters=best_params['n_clusters'], random_state=42)\n",
    "        elif algo == 'hdbscan':\n",
    "            model = hdbscan.HDBSCAN(min_cluster_size=best_params['min_cluster_size'])\n",
    "        elif algo == 'spectral':\n",
    "            model = SpectralClustering(n_clusters=best_params['n_clusters'], \n",
    "                                       affinity='nearest_neighbors', random_state=42)\n",
    "        elif algo == 'agglomerative':\n",
    "            model = AgglomerativeClustering(n_clusters=best_params['n_clusters'])\n",
    "        elif algo == 'gmm':\n",
    "            model = GaussianMixture(n_components=best_params['n_components'], random_state=42)\n",
    "\n",
    "        print(f\"Fitting {algo} model...\")\n",
    "        cluster_labels_list.append(model.fit_predict(X))\n",
    "\n",
    "    print(\"\\nCreating consensus clustering...\")\n",
    "    n_samples = X.shape[0]\n",
    "    co_assoc_matrix = np.zeros((n_samples, n_samples))\n",
    "    for labels in cluster_labels_list:\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                if labels[i] == labels[j]:\n",
    "                    co_assoc_matrix[i, j] += 1\n",
    "    co_assoc_matrix /= len(algorithms)\n",
    "\n",
    "    print(\"\\nOptimizing linkage...\")\n",
    "    best_linkage_params = optimize_linkage(co_assoc_matrix)\n",
    "    print(f\"Best linkage parameters: {best_linkage_params}\")\n",
    "\n",
    "    linkage_matrix = linkage(1 - co_assoc_matrix, method=best_linkage_params['linkage_method'])\n",
    "    final_labels = fcluster(linkage_matrix, t=best_linkage_params['n_clusters'], criterion='maxclust')\n",
    "\n",
    "    print(\"\\nEvaluating Clustering Performance:\")\n",
    "    silhouette = silhouette_score(1 - co_assoc_matrix, final_labels)\n",
    "    davies_bouldin = davies_bouldin_score(1 - co_assoc_matrix, final_labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(1 - co_assoc_matrix, final_labels)\n",
    "    dunn = dunn_index(1 - co_assoc_matrix, final_labels)\n",
    "\n",
    "    print(f\"Silhouette Score      : {silhouette:.4f}\")\n",
    "    print(f\"Davies-Bouldin Index  : {davies_bouldin:.4f}\")\n",
    "    print(f\"Calinski-Harabasz     : {calinski_harabasz:.4f}\")\n",
    "    print(f\"Dunn Index            : {dunn:.4f}\")\n",
    "\n",
    "    data['Consensus_Cluster'] = final_labels\n",
    "    \n",
    "    cluster_stats = pd.DataFrame({\n",
    "        'Cluster': range(1, best_linkage_params['n_clusters'] + 1),\n",
    "        'Size': [sum(final_labels == i) for i in range(1, best_linkage_params['n_clusters'] + 1)]\n",
    "    })\n",
    "    print(\"\\nCluster Statistics:\")\n",
    "    print(cluster_stats)\n",
    "\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_path = csv_path.replace('.csv', f'_clustered_{today}.csv')\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f\"\\nClustered data saved to: {output_path}\")\n",
    "\n",
    "    return final_labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = r\"Drugs_With_Embedded_Bits.csv\"\n",
    "    final_clusters = perform_ensemble_clustering(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "csv_path = r\"Drugs_With_Embedded_clustered_2025-02-13.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "if 'Consensus_Cluster' not in df.columns:\n",
    "    raise ValueError(\"CSV file must contain a column named 'Consensus_Cluster'.\")\n",
    "\n",
    "numerical_data = df.select_dtypes(include=[np.number])\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(numerical_data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "cluster_labels = df['Consensus_Cluster']\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=cluster_labels, palette=\"tab10\", s=80, edgecolor=\"black\")\n",
    "plt.title(\"PCA Projection of Clusters\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pd.crosstab(df['Consensus_Cluster'], df['Consensus_Cluster']), annot=True, cmap=\"coolwarm\", fmt=\"d\")\n",
    "plt.title(\"Cluster Distribution Heatmap\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.ylabel(\"Cluster ID\")\n",
    "plt.show()\n",
    "\n",
    "num_features = numerical_data.columns[:6]\n",
    "df_imputed = pd.DataFrame(X_imputed, columns=numerical_data.columns)\n",
    "df_imputed['Consensus_Cluster'] = cluster_labels\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(num_features):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.boxplot(x='Consensus_Cluster', y=feature, data=df_imputed, palette=\"Set2\")\n",
    "    plt.title(f\"Boxplot of {feature} by Cluster\")\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sns.pairplot(df_imputed[num_features.tolist() + ['Consensus_Cluster']], hue=\"Consensus_Cluster\", palette=\"tab10\", diag_kind=\"kde\")\n",
    "plt.suptitle(\"Pairplot of Clusters\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=cluster_labels, palette=\"viridis\")\n",
    "plt.title(\"Cluster Size Distribution\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(\"All visualizations generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61f9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
